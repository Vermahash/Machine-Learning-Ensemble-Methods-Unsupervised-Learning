{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818783e2-9a92-4e94-893e-8bfb14f89180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Hofstra Assignements\\ML\\Project3.complete\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "E:\\Hofstra Assignements\\ML\\Project3.complete\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Check the output_plots directory for results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import umap.umap_ as umap\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = pd.read_csv('dataset_to_447109517.csv').drop(columns=['name'], errors='ignore')\n",
    "dataset2 = pd.read_csv('kaggle_Int_755833365.csv').drop(columns=['name'], errors='ignore')\n",
    "\n",
    "# Data Preprocessing Functions\n",
    "def drop_high_missing_columns(data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Drop columns with missing values exceeding a specified threshold.\n",
    "    \"\"\"\n",
    "    missing_ratios = data.isnull().mean()\n",
    "    columns_to_drop = missing_ratios[missing_ratios > threshold].index\n",
    "    print(f\"Dropping columns with more than {threshold*100}% missing values: {columns_to_drop.tolist()}\")\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    return data\n",
    "\n",
    "def encode_non_numeric(data):\n",
    "    \"\"\"\n",
    "    Encode non-numeric columns using LabelEncoder.\n",
    "    \"\"\"\n",
    "    non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in non_numeric_columns:\n",
    "        data[col] = label_encoder.fit_transform(data[col].astype(str))\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data, columns_to_scale=None, categorical_columns=None, scaling_method='standard'):\n",
    "    \"\"\"\n",
    "    Preprocess data with encoding for non-numeric columns and scaling.\n",
    "    \"\"\"\n",
    "    # Drop columns with excessive missing values\n",
    "\n",
    "    # Encode non-numeric columns\n",
    "    data = encode_non_numeric(data)\n",
    "\n",
    "    # Fill missing values for numeric columns\n",
    "    if columns_to_scale:\n",
    "        valid_columns = [col for col in columns_to_scale if col in data.columns]\n",
    "        data[valid_columns] = data[valid_columns].fillna(data[valid_columns].mean())\n",
    "\n",
    "    # Fill missing values for categorical columns\n",
    "    if categorical_columns:\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "    # Scale numeric columns\n",
    "    if columns_to_scale:\n",
    "        scaler = None\n",
    "        if scaling_method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling_method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling_method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method. Choose 'standard', 'minmax', or 'robust'.\")\n",
    "        \n",
    "        valid_columns = [col for col in columns_to_scale if col in data.columns]\n",
    "        data[valid_columns] = scaler.fit_transform(data[valid_columns])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Preprocess both datasets\n",
    "columns_to_scale1 = dataset1.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "columns_to_scale2 = dataset2.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "dataset1 = preprocess_data(dataset1, columns_to_scale=columns_to_scale1)\n",
    "columns_to_scale1 = dataset1.select_dtypes(include=['float64', 'int64']).columns.tolist()  # Recalculate after preprocessing\n",
    "dataset2 = preprocess_data(dataset2, columns_to_scale=columns_to_scale2)\n",
    "columns_to_scale2 = dataset2.select_dtypes(include=['float64', 'int64']).columns.tolist()  # Recalculate after preprocessing\n",
    "\n",
    "# PCA\n",
    "pca_results = {}\n",
    "for dataset, name in zip([dataset1, dataset2], ['Dataset 1', 'Dataset 2']):\n",
    "    numeric_data = dataset.select_dtypes(include=['float64', 'int64']).dropna()\n",
    "    if numeric_data.empty:\n",
    "        print(f\"Skipping PCA for {name} as no numeric data is available.\")\n",
    "        continue\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(StandardScaler().fit_transform(numeric_data))\n",
    "\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "    pca_results[name] = pca_result\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, label='Individual Variance')\n",
    "    plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative Variance')\n",
    "    plt.title(f\"{name} - PCA Explained Variance\")\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Variance Ratio')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# UMAP\n",
    "umap_results = {}\n",
    "for dataset, name in zip([dataset1, dataset2], ['Dataset 1', 'Dataset 2']):\n",
    "    numeric_data = dataset.select_dtypes(include=['float64', 'int64']).dropna()\n",
    "    if numeric_data.empty:\n",
    "        print(f\"Skipping UMAP for {name} as no numeric data is available.\")\n",
    "        continue\n",
    "    umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "    umap_result = umap_model.fit_transform(StandardScaler().fit_transform(numeric_data))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(umap_result[:, 0], umap_result[:, 1], s=10, alpha=0.7)\n",
    "    plt.title(f\"{name} - UMAP 2D\")\n",
    "    plt.show()\n",
    "\n",
    "    umap_results[name] = umap_result\n",
    "\n",
    "# Elbow and Silhouette Scores with Dendrograms\n",
    "for dataset, name, pca_data, umap_data in zip([dataset1, dataset2], ['Dataset 1', 'Dataset 2'], pca_results.values(), umap_results.values()):\n",
    "    for data, data_name in zip([dataset.select_dtypes(include=['float64', 'int64']), pca_data, umap_data], ['Original', 'PCA-Reduced', 'UMAP-Reduced']):\n",
    "        scaled_data = StandardScaler().fit_transform(data)\n",
    "\n",
    "        # Elbow Method\n",
    "        inertias = []\n",
    "        silhouettes = []\n",
    "        for n_clusters in range(2, 11):\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            labels = kmeans.fit_predict(scaled_data)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            silhouettes.append(silhouette_score(scaled_data, labels))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(range(2, 11), inertias, marker='o', label='Inertia')\n",
    "        plt.title(f\"{name} - {data_name} Data - Elbow Plot\")\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(range(2, 11), silhouettes, marker='o', label='Silhouette Score')\n",
    "        plt.title(f\"{name} - {data_name} Data - Silhouette Scores\")\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Dendrogram for Hierarchical Clustering\n",
    "        linkage_matrix = linkage(scaled_data, method='ward')\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(linkage_matrix)\n",
    "        plt.title(f\"{name} - {data_name} Data - Hierarchical Clustering Dendrogram\")\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "\n",
    "        # K-Means Clusters Plot\n",
    "        optimal_k = 4  # Replace with the determined optimal number of clusters\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=labels, cmap='viridis', s=10)\n",
    "        plt.title(f\"{name} - {data_name} Data - K-Means Clustering\")\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.colorbar(label='Cluster')\n",
    "        plt.show()\n",
    "# Evaluation for Original, PCA, and UMAP Data\n",
    "for dataset, name, target_column, pca_data, umap_data in zip([dataset1, dataset2], ['Dataset 1', 'Dataset 2'], ['PROFILE', 'group'], pca_results.values(), umap_results.values()):\n",
    "    if target_column in dataset.columns:\n",
    "        X = dataset.drop(columns=[target_column])\n",
    "        y = dataset[target_column]\n",
    "\n",
    "        for data, data_name in zip([X, pca_data, umap_data], ['Original', 'PCA-Reduced', 'UMAP-Reduced']):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "            ada_model = AdaBoostClassifier(n_estimators=100, random_state=42, algorithm=\"SAMME\")\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "            # Train and evaluate AdaBoost\n",
    "            ada_model.fit(X_train, y_train)\n",
    "            y_test_pred_ada = ada_model.predict(X_test)\n",
    "            y_test_proba_ada = ada_model.predict_proba(X_test)[:, 1] if hasattr(ada_model, \"predict_proba\") else None\n",
    "\n",
    "            # Train and evaluate Random Forest\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            y_test_pred_rf = rf_model.predict(X_test)\n",
    "            y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1] if hasattr(rf_model, \"predict_proba\") else None\n",
    "\n",
    "            # Metrics\n",
    "            metrics = pd.DataFrame([\n",
    "                [\"AdaBoost\", data_name, accuracy_score(y_test, y_test_pred_ada),\n",
    "                 f1_score(y_test, y_test_pred_ada, average='weighted'),\n",
    "                 precision_score(y_test, y_test_pred_ada, average='weighted'),\n",
    "                 recall_score(y_test, y_test_pred_ada, average='weighted')],\n",
    "                [\"Random Forest\", data_name, accuracy_score(y_test, y_test_pred_rf),\n",
    "                 f1_score(y_test, y_test_pred_rf, average='weighted'),\n",
    "                 precision_score(y_test, y_test_pred_rf, average='weighted'),\n",
    "                 recall_score(y_test, y_test_pred_rf, average='weighted')]\n",
    "            ], columns=[\"Model\", \"Data\", \"Accuracy\", \"F1-Score\", \"Precision\", \"Recall\"])\n",
    "\n",
    "            print(f\"\\nMetrics for {name} - {data_name} Data:\")\n",
    "            print(metrics)\n",
    "\n",
    "            # Confusion Matrices\n",
    "            for model, model_name, y_pred in zip(\n",
    "                [ada_model, rf_model], [\"AdaBoost\", \"Random Forest\"], [y_test_pred_ada, y_test_pred_rf]\n",
    "            ):\n",
    "                cm = confusion_matrix(y_test, y_pred)\n",
    "                plt.figure()\n",
    "                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "                plt.title(f\"{model_name} - Confusion Matrix ({data_name})\")\n",
    "                plt.xlabel(\"Predicted\")\n",
    "                plt.ylabel(\"Actual\")\n",
    "                plt.show()\n",
    "\n",
    "            # ROC Curve\n",
    "            if len(y.unique()) == 2:  # Binary classification\n",
    "                plt.figure()\n",
    "                for model, label, y_proba in zip(\n",
    "                    [ada_model, rf_model],\n",
    "                    [\"AdaBoost\", \"Random Forest\"],\n",
    "                    [y_test_proba_ada, y_test_proba_rf],\n",
    "                ):\n",
    "                    if y_proba is not None:\n",
    "                        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                        auc_score = roc_auc_score(y_test, y_proba)\n",
    "                        plt.plot(fpr, tpr, label=f\"{label} (AUC = {auc_score:.2f})\")\n",
    "                plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "                plt.title(f\"ROC Curve ({data_name})\")\n",
    "                plt.xlabel(\"False Positive Rate\")\n",
    "                plt.ylabel(\"True Positive Rate\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "            # K-Means Clusters Plot\n",
    "            optimal_k = 4  # Replace with the determined optimal number of clusters\n",
    "            kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "            labels = kmeans.fit_predict(X_test)\n",
    "\n",
    "            # Ensure compatibility for plotting\n",
    "            if isinstance(X_test, pd.DataFrame):\n",
    "                X_test_array = X_test.values  # Convert to NumPy array\n",
    "            else:\n",
    "                X_test_array = X_test\n",
    "\n",
    "            if X_test_array.shape[1] >= 2:  # Check for at least two dimensions\n",
    "                plt.figure()\n",
    "                plt.scatter(X_test_array[:, 0], X_test_array[:, 1], c=labels, cmap='viridis', s=10)\n",
    "                plt.title(f\"{name} - {data_name} Data - K-Means Clustering\")\n",
    "                plt.xlabel('Feature 1')\n",
    "                plt.ylabel('Feature 2')\n",
    "                plt.colorbar(label='Cluster')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Insufficient dimensions for K-Means plot: {name} - {data_name} Data.\")\n",
    "\n",
    "            # Dendrogram for Hierarchical Clustering\n",
    "            linkage_matrix = linkage(X_test_array, method='ward')\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            dendrogram(linkage_matrix)\n",
    "            plt.title(f\"{name} - {data_name} Data - Hierarchical Clustering Dendrogram\")\n",
    "            plt.xlabel(\"Sample Index\")\n",
    "            plt.ylabel(\"Distance\")\n",
    "            plt.show()\n",
    "\n",
    "            # Silhouette Score and Elbow Plot\n",
    "            inertias = []\n",
    "            silhouettes = []\n",
    "            for n_clusters in range(2, 11):\n",
    "                kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                cluster_labels = kmeans_model.fit_predict(X_test_array)\n",
    "                inertias.append(kmeans_model.inertia_)\n",
    "                silhouettes.append(silhouette_score(X_test_array, cluster_labels))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(range(2, 11), inertias, marker='o', label='Inertia')\n",
    "            plt.title(f\"{name} - {data_name} Data - Elbow Plot\")\n",
    "            plt.xlabel(\"Number of Clusters\")\n",
    "            plt.ylabel(\"Inertia\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(range(2, 11), silhouettes, marker='o', label='Silhouette Score')\n",
    "            plt.title(f\"{name} - {data_name} Data - Silhouette Scores\")\n",
    "            plt.xlabel(\"Number of Clusters\")\n",
    "            plt.ylabel(\"Silhouette Score\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189abd6c-83c7-4eda-8eb2-6e7b6e1e80ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
